{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dad8e9abba327408",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    \n",
    "    if example['Speaker'] == \"Any\":\n",
    "        npc_type = \"any type of NPC\"\n",
    "    else:\n",
    "        npc_type = f'the NPC type {example[\"Speaker\"]}'\n",
    "    \n",
    "    if example['Event'] == \"Greeting\":\n",
    "        \n",
    "        return f\"\"\"In the world of Skyrim from the game Elder Scrolls V, in the case of a {example['Event']}, between an NPC and the Player character, {npc_type} greets the player with the following dialogue: {example['Dialogue']}, in a {example['Tone']} tone.\"\"\"\n",
    "    \n",
    "    \n",
    "    if example['Event'] == \"Goodbye\":\n",
    "        \n",
    "        return f\"\"\"In the world of Skyrim from the game Elder Scrolls V, in the case of a {example['Event']}, between an NPC and the Player character,{npc_type} tells their goodbyes to the Player with the following dialogue: {example['Dialogue']} \"\"\"\n",
    "    \n",
    "    \n",
    "    if example['Event'] == \"NPCs fighting over a player's dropped item\":\n",
    "        \n",
    "        return f\"\"\"In the world of Skyrim from the game Elder Scrolls V, in the event of {example['Event']}, there can be up to 5 different NPCs arguing over an item\n",
    "        that the player character dropped and 2 bystander NPCs. Up to five lines of dialogue can be exchanged between up to five different types of NPCs, meaning that the conversation ends after 5 lines of dialogue have been said.\n",
    "        \n",
    "        In this case one of the NPCs that takes part in the arguing and is {npc_type} says the following line: {example['Dialogue']} as the\n",
    "        {example['Line_of_dialogue']} line of the dialogue.\"\"\"\n",
    "    \n",
    "    if example['Event'] == \"NPC asking for the Player's dropped armor\":\n",
    "        \n",
    "        if example['Speaker'] == \"Player\":\n",
    "        \n",
    "            return f\"\"\"In the world of Skyrim from the game Elder Scrolls V, in the event of an {example['Event']} there is a conversation between the Player character and an NPC about the player's dropped item. \n",
    "            \n",
    "            In this case the Player character says the following line: {example['Dialogue']} in response to the NPC's previous line of dialogue: {example['Response_to']}.\"\"\"\n",
    "        \n",
    "        if example['Response_to'] == \"nan\":\n",
    "            \n",
    "            return f\"\"\"In the world of Skyrim from the game Elder Scrolls V, in the event of {example['Event']} there is a conversation between the Player character and an NPC about the player's dropped item. \n",
    "            \n",
    "            In this case {npc_type} says the following line: {example['Dialogue']} in response to the Player character's previous line of dialogue: {example['Response_to']}.\"\"\"\n",
    "        else:\n",
    "            \n",
    "            return f\"\"\"In the world of Skyrim from the game Elder Scrolls V, in the event of {example['Event']} there is a conversation between the Player character and an NPC about the player's dropped item. \n",
    "            \n",
    "            In this case {npc_type} says the following line: {example['Dialogue']} to initiate a conversation about the Player's dropped item.\"\"\"\n",
    "    \n",
    "    if example['Condition'] == \"in combat\":\n",
    "        \n",
    "        return f\"\"\"In the world of Skyrim from the game Elder Scrolls V, when the Player character and a friendly NPC are {example['Condition']}, {npc_type} uses this line of dialogue: {example['Dialogue']}, when {example['Event']}\"\"\"\n",
    "        \n",
    "    if example['Result'] is not None:\n",
    "        \n",
    "            return f\"\"\"In the world of Skyrim from the game Elder Scrolls V, when the Player character tries to check if an NPC of any type can be persuaded, the NPC uses the following line of dialogue: {example['Dialogue']} when {example['Event']} and as a result {example['Result']} \"\"\"\n",
    "    \n",
    "    if example['Event'] == \"an NPC spots the Player character having an amulet of mara\":\n",
    "        \n",
    "        if example['Speaker'] == \"Player\":\n",
    "            \n",
    "            return f\"\"\"In the world of Skyrim from the game Elder Scrolls V, when {example['Event']} an interaction begins between the two about the possibility of marriage. \n",
    "            \n",
    "            In this case the Player character says the following line: {example['Dialogue']} as a response to the the NPC's dialogue line:{example['Response_to']}\"\"\"\n",
    "        \n",
    "        if example['Response_to'] == \"nan\":\n",
    "            \n",
    "            return f\"\"\"In the world of Skyrim from the game Elder Scrolls V, when {example['Event']} an interaction begins between the two about the possibility of marriage.\n",
    "                \n",
    "            In this case {npc_type} says the following line: {example['Dialogue']} to initiate the conversation about marriage.\"\"\"\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            return f\"\"\"In the world of Skyrim from the game Elder Scrolls V, when {example['Event']} an interaction begins between the two about the possibility of marriage. \n",
    "            \n",
    "            In this case {npc_type} says the following line: {example['Dialogue']} as a response to the the Player's dialogue line:{example['Response_to']}\"\"\"\n",
    "    \n",
    "    if example['Race'] is not None:\n",
    "        \n",
    "        if example['Condition'] != \"nan\":\n",
    "            \n",
    "            return f\"\"\"In the world of Skyrim from the game Elder Scrolls V, Town Guards have special reactions to the race of the Player character, so in the case of {example['Event']} if the Player's race is: {example['Race']} the Guard reacts with this line of dialogue: {example['Dialogue']} on the condition that the {example['Condition']}. \"\"\"\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            return f\"\"\"In the world of Skyrim from the game Elder Scrolls V, Town Guards have special reactions to the race of the Player character, so in the case of {example['Event']} if the Player's race is: {example['Race']} the Guard reacts with this line of dialogue: {example['Dialogue']}. \"\"\"\n",
    "     \n",
    "    if example['Equipment'] is not None:\n",
    "        \n",
    "        if example['Condition'] != \"nan\":\n",
    "            \n",
    "            return f\"\"\"In the world of Skyrim from the game Elder Scrolls V, Town Guards have special reactions to the equipment the player is holding or has equipped, so when in the case of a {example['Event']}, the Guard uses this line of dialogue:{example['Dialogue']} on the condition that {example['Condition']}.\"\"\"\n",
    "        \n",
    "        else: \n",
    "            \n",
    "            return f\"\"\"In the world of Skyrim from the game Elder Scrolls V, Town Guards have special reactions to the equipment the player is holding or has equipped, so when in the case of a {example['Event']}, the Guard uses this line of dialogue:{example['Dialogue']}.\"\"\"\n",
    "            \n",
    "    if example['Location'] is not None:\n",
    "        \n",
    "        if example['Condition'] != \"nan\":\n",
    "            \n",
    "            return f\"\"\"In the world of Skyrim from the game Elder Scrolls V, Town Guards have special interactions with the Player character that are location based, meaning that the Guards sometimes interact with the Player using town specific dialogue.\n",
    "\n",
    "            In this case a Guard in the town of {example['Location']} might use this line of dialogue {example['Dialogue']} when the Player interacts with them, on the condition that {example['Condition']}.\"\"\"\n",
    "        \n",
    "        else: \n",
    "            \n",
    "            return f\"\"\"In the world of Skyrim from the game Elder Scrolls V, Town Guards have special interactions with the Player character that are location based, meaning that the Guards sometimes interact with the Player using town specific dialogue.\n",
    "\n",
    "            In this case a Guard in the town of {example['Location']} might use this line of dialogue {example['Dialogue']} when the Player interacts with them.\"\"\"\n",
    "        \n",
    "    if example['Condition'] is not None:\n",
    "        \n",
    "        if example['Condition'] != \"nan\":\n",
    "            \n",
    "            return f\"\"\"In the world of Skyrim from the game Elder Scrolls V, in the event of {example['Event']}, {npc_type} says the following line to the Player ch+aracter: {example['Dialogue']} on the condition that {example['Condition']}.\"\"\"\n",
    "    \n",
    "    return f\"\"\"In the world of Skyrim from the game Elder Scrolls V, in the event of {example['Event']}, {npc_type} says the following line to the Player character: {example['Dialogue']}.\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "131418b6535338bf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfe4ef3ec3c37141",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6438a2ecf446de0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset['train']]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset['train']]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe34416de1121bf2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "max_length = 120  # This was an appropriate max length for my dataset\n",
    "\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b3e76e5d59b83b4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4121ff13e365288",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(tokenized_train_dataset['train'][1]['input_ids'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "272efb57d867178d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5673a5175e6f2638",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "eval_prompt = (\"Start a conversation where Dutch van der Linde plans a new heist Stealing the USA gold reserves\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "678f82c73601a318",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Init an eval tokenizer that doesn't add padding or eos token\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0],\n",
    "                                skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47c8339c7d3e3ea2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from peft import PeftConfig\n",
    "\n",
    "adapter_model_id = \"ybelkada/opt-350m-lora\"\n",
    "peft_config = PeftConfig.from_pretrained(adapter_model_id)\n",
    "# to initiate with random weights\n",
    "peft_config.init_lora_weights = False\n",
    "\n",
    "model.add_adapter(peft_config)\n",
    "model.enable_adapters()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2441ddcd1a5181df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(\"###Character: \" + test_dataset['train'][200]['character'] + \"\\n  responded with: \" + test_dataset['train'][200][\n",
    "    'dialogue'] + \"\\n to what was said by: \" + test_dataset['train'][200]['response_to'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ab714e941181fd6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "570b93f08b1b61eb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddf562f6b54c4893",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fe694a164bb1ea8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6354fba6c3915b3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:  # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fcbe8317af8659a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = accelerator.prepare_model(model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec3566a59128b91",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"own-finetune\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_val_dataset[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=200,\n",
    "        learning_rate=2.5e-5,  # Want a small lr for finetuning\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=25,  # When to start reporting loss\n",
    "        logging_dir=\"./logs\",  # Directory for storing logs\n",
    "        save_strategy=\"steps\",  # Save the model checkpoint every logging step\n",
    "        save_steps=25,  # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\",  # Evaluate the model every logging step\n",
    "        eval_steps=25,  # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,\n",
    "        # Perform evaluation at the end of training         # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"  # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c48d66dfa4929c8f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea90e93a904c5d4a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"mistral-own-finetune/checkpoint-50\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e154dc87f95c4f41",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "eval_prompt = \"Start a conversation where Dutch van der Linde plans a new heist\"\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=280, repetition_penalty=1.15)[0],\n",
    "                                skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16f5ddad06c7fa1b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "aa52a4ae46dfb31"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "eval_prompt = \"Imagine a scenario in Red Dead Redemption 2 where Abe informs John Marston that there is a courier outside of his ranch waiting to deliver him a telegram. John Marston is intrigued because he was not expecting one and makes a jokes about the contents of the telegram and then follows abe to the courier, finally he asks if the courier has a telegram for him. Generate the dialogue John and Abe during this scenario.\"\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=162, repetition_penalty=1.15)[0],\n",
    "                                skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfab4fdc616407b6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "eval_prompt = \"Imagine a scenario in Red Dead Redemption 2 where Abe informs John Marston that there is a courier outside of his ranch waiting to deliver him a telegram. John Marston is intrigued because he was not expecting one and makes a jokes about the contents of the telegram and then follows abe to the courier, finally he asks if the courier has a telegram for him. Generate the dialogue John and Abe during this scenario.\"\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=170, repetition_penalty=1.15)[0],\n",
    "                                skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab24bb42ffe34fee",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"\n",
    "\n",
    "[INST]Continue the last 2 lines of conversation between Arthur Morgan and a doctor in Red Dead Redemption 2 and not any further [/INST]\n",
    "\n",
    "Doctor: Sure, thank you. Now, what’s wrong? I mean, what appear to be the symptoms?\n",
    "\n",
    "Arthur Morgan: Well, I think you’ve heard them… I’m, I’m coughing.\n",
    "\n",
    "Doctor: Is there any blood?\n",
    "\n",
    "Arthur Morgan: Sometimes.\n",
    "\n",
    "Doctor: Okay now, here… breathe. Again. Let me see your tongue. Now say ahh.\n",
    "\n",
    "Arthur Morgan: Aah… What is it?\n",
    "\n",
    "Doctor: It’s not good news.\n",
    "\n",
    "Arthur Morgan: Well I guessed that.\n",
    "\n",
    "Doctor: \"\"\"\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=43, repetition_penalty=1.15)[0],\n",
    "                                skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "234ce363389ca88f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c21d6c4c9c86806c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Rouge Score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bef0d5c5cee19e4f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "generated_dialogues = [\"Doctor:  I'm afraid you have tuberculosis. We need to start treatment right away.\",\n",
    "                       \"Arthur Morgan: Tuberculosis... That's a death sentence isn't it?\"]\n",
    "\n",
    "reference_dialogues = [\"Doctor: You got tuberculosis. I’m really sorry for you, son, it’s a hell of a thing.\",\n",
    "                       \"Arthur Morgan: Aah… What is it?\"]\n",
    "\n",
    "print(len(reference_dialogues))\n",
    "\n",
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "rouge_scores = rouge.get_scores(' '.join(generated_dialogues), ' '.join(reference_dialogues[0]))\n",
    "\n",
    "# Output the scores \n",
    "\n",
    "print(f\"ROUGE: {rouge_scores}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9c66b4e9da55ee4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "\n",
    "# Tokenize the reference and generated dialogues into words\n",
    "reference_dialogues = [\n",
    "    [\"Doctor:\", \"You\", \"got\", \"tuberculosis.\", \"I’m\", \"really\", \"sorry\", \"for\", \"you,\", \"son,\", \"it’s\", \"a\", \"hell\", \"of\", \"a\", \"thing.\"],\n",
    "    [\"Arthur\", \"Morgan:\", \"Aah…\", \"What\", \"is\", \"it?\"]\n",
    "]\n",
    "generated_dialogues = [\n",
    "    [\"Doctor:\", \"I'm\", \"afraid\", \"you\", \"have\", \"tuberculosis.\", \"We\", \"need\", \"to\", \"start\", \"treatment\", \"right\", \"away.\"],\n",
    "    [\"Arthur\", \"Morgan:\", \"Tuberculosis...\", \"That's\", \"a\", \"death\", \"sentence\", \"isn't\", \"it?\"]\n",
    "]\n",
    "\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = sentence_bleu(reference_dialogues, generated_dialogues)\n",
    "print(f\"BLEU: {bleu_score}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b738655740170ad",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "728c8d2a5290e8c7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
